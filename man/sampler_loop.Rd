% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sampler_loop.R
\name{sampler_loop}
\alias{sampler_loop}
\title{Generate an imputed data set via iterative procedure.}
\usage{
sampler_loop(
  data,
  model,
  indicator,
  call_lr_train,
  sampler,
  prediction_type,
  stop_measure = measure_correlation,
  loop_limit = 10L,
  overrides = list(),
  clean_step = list(),
  chain_id = 1L,
  verbose = FALSE
)
}
\arguments{
\item{model}{matrix;
logical matrix which indicates inclusion of a predictor (named
column) in the model of an imputed value (named row), with the
order of imputation being the row order, default is a matrix of
ones with rows for each partially but not-completely missing
variable (in order of least to most missing), and columns
for every partially complete variable.}

\item{call_lr_train}{call: a 'skeleton' call to \code{\link[literanger:train]{literanger::train()}} for
fitting random forests, arguments can be over-ridden on a per-variable basis
by \code{overrides}.}

\item{overrides}{named list;
(variable-wise) over-rides for arguments passed to
\code{\link[ranger]{ranger}} when training on the response
variable given by the name of the item.}

\item{chain_id}{integer (scalar): the identifier of the current chain to be
displayed in the progress bar.}

\item{verbose}{logical;
print additional output.}
}
\value{
named list: the following results from the iterative procedure:
\describe{
\item{\code{converged}}{logical: indicator of convergence.}
\item{\code{imputed}}{list: each item contains a named list of imputed values
(for each variable) from one completed iteration; values are provided in
the order that missing cases appear in the data (per variable).}
\item{\code{iterations}}{integer: the number of completed iterations.}
\item{\code{oob_error}}{data.frame: per-variable and per-iteration out-of-bag
error estimates, with columns \code{iteration}, \code{variable}, \code{measure} (either
'pfc' for categorical data, or 'mse' for continuous), and \code{value}.}
\item{\code{stop_measures}}{list: each item contains a named numeric vector with
the stopping criterion measures from one completed iteration.}
}
}
\description{
A similar iterative procedure spans the process for either a single chain
for the Multiple Imputation by Chained Equations algorithm, a.k.a. 'mice'
(van Buuren and Groothuis-Oudshoorn, 2011) and the process of estimation
by 'missForest' (Stekhoven and Buehlmann, 2012). Each step of the procedure
involves fitting (sequentially) a random forest to each variable and then
imputing new values for the missing cases. The two differ by the type of
prediction used to generate a new value, and by whether the latest imputed
values are used in training or exclusively those of the preceding step.
}
\details{
For a full description of the procedure for a single chain of 'mice' using
random forests see Doove et al (2014) and Bartlett (2014). In brief, the
procedure is:
\itemize{
\item For each variable (in a pre-specified order):
\itemize{
\item Train a random forest using the observed values.
\item For each missing case, traverse one or more trees and identify a pool
of candidate values from the leaves (for Doove et al, take the
observed from each leaf, for Bartlett et al, take bootstrap observed
values from one random leaf).
\item Draw once from the candidate values and update the variable.
}
}

Here, \code{data} must be complete; i.e. any missing value has already been
filled in some naive way using functions like \code{\link[=impute_naive_by_sample]{impute_naive_by_sample()}}
(the usual for 'mice') or \code{\link[=impute_naive_by_aggregate]{impute_naive_by_aggregate()}} (the original
'missForest' approach). \code{indicator} will dictate which values are considered
missing cases and will be updated by the loop.

\code{model} identifies which variables are included in each random forest. This
is either a numeric (with zero and one values) or logical matrix. See
\code{\link[=smirf]{smirf()}} for further details.

Training is performed by a call to \code{\link[literanger:train]{literanger::train()}} given by
\code{call_lr_train}, which may contain arguments applied to all random
forests. Per-variable arguments can be supplied using \code{overrides} which will
replace the 'global' arguments in \code{call_lr_train}. By default, classification
trees are used for factors, while regression trees are used for continuous
data.

\code{sampler} identifies whether or not a Gibbs-like sampler is employed
(\verb{='gibbs'}) over the variables or a 'missForest'-like sampler
(\verb{='missforest'}). The Gibbs-like sampler trains on the latest imputed
values, whereas the 'missForest'-like trains on the imputed values from the
previous iteration's complete data set. \code{prediction_type} set to \code{'inbag'}
uses Bartlett's prediction (2014), while \code{'doove'} uses the approach from
'mice' by Doove et al (2014). \code{'bagged'} can be used if estimating missing
values ala 'missForest'.

\code{stop_measure} can be set to one of \code{\link[=measure_correlation]{measure_correlation()}},
\code{\link[=measure_stekhoven_2012]{measure_stekhoven_2012()}}, or \code{\link[=measure_degenerate]{measure_degenerate()}}. The latter is
applicable to 'mice'-like algorithms and stops the loop when it reaches the
pre-determined \code{loop_limit}, while the former two are variations on stopping
criteria that can be used in 'missForest'-like estimation.

\code{clean_step} is a per-variable post-processing function called on each
variable after it has been imputed. It can be used to ensure that constraints
are applied to variables. Each item in the list is a function that accepts
\code{data} and \code{imputed}; where the former is the data-set prior to imputed
values being drawn (or the preceding complete data set if
\code{sampler='missforest'}) restricted to the missing cases (in order). The
function should return a vector the same length and type as \code{imputed} with
the clean values.

All the imputed values (over all iterations) are returned, along with; a
convergence indicator (if applicable); the number of iterations performed;
the out-of-bag error on a per-iteration and per-variable basis, and; the
recorded values of the stopping condition measures for each iteration.
}
\references{
\itemize{
\item Bartlett, J. (2014, 6-11 July). \emph{Methodology for multiple imputation for
missing data in electronic health record data} \link{Conference presentation}.
International Biometric Conference, Florence, TOS, Italy.
\href{https://web.archive.org/web/20190819140612/http://thestatsgeek.com/wp-content/uploads/2014/09/RandomForestImpBiometricsConf.pdf}{Archived 2019-08-19}.
\item Stekhoven, D. J. & Buehlmann, P. (2012). MissForest--non-parametric
missing value imputation for mixed-type data. \emph{Bioinformatics}, 28(1),
112-118. \doi{10.1093/bioinformatics/btr597}.
\item Van Buuren, S. & Groothuis-Oudshoorn, K. (2011). mice: Multivariate
Imputation by Chained Equations in R. \emph{Journal of Statistical Software},
\emph{45}(3), 1-67. \doi{10.18637/jss.v045.i03}.
}
}
