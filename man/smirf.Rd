% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/smirf.R
\name{smirf}
\alias{smirf}
\title{A similar iterative procedure spans the process for either a single chain
for the Multiple Imputation by Chained Equations algorithm, a.k.a. 'mice'
(van Buuren and Groothuis-Oudshoorn, 2011) and the process of estimation
by 'missForest' (Stekhoven and Buehlmann, 2012). Each step of the procedure
involves fitting (sequentially) a random forest to each variable and then
imputing new values for the missing cases. The two differ by the type of
prediction used to generate a new value, and by whether the latest imputed
values are used in training or exclusively those of the preceding step.}
\usage{
smirf(
  data,
  model = NULL,
  n = 5L,
  sampler = c("gibbs", "missforest"),
  prediction_type = c("inbag", "bagged", "doove"),
  fn_init = impute_naive_by_sample,
  stop_measure = measure_degenerate,
  loop_limit = 10L,
  overrides = list(),
  clean_step = list(),
  verbose = FALSE,
  ...
)
}
\arguments{
\item{model}{matrix;
logical matrix which indicates inclusion of a predictor (named
column) in the model of an imputed value (named row), with the
order of imputation being the row order, default is a matrix of
ones with rows for each partially but not-completely missing
variable (in order of least to most missing), and columns
for every partially complete variable.}

\item{n}{numeric scalar;
the number of imputations - i.e. number of times the missForest
algorithm is used.}

\item{overrides}{named list;
(variable-wise) over-rides for arguments passed to
\code{\link[ranger]{ranger}} when training on the response
variable given by the name of the item.}

\item{verbose}{logical;
print additional output.}

\item{...}{further arguments passed to all calls to
\code{\link[ranger]{ranger}}, e.g. \code{num.trees} for the number
of trees in each forest.}

\item{X}{data.frame;
a incomplete data set including any of numeric, logical, integer,
factor and ordered data types.}

\item{gibbs}{logical;
use Gibbs sampling in training steps (\code{T}) rather than the
predictions from the previous iteration (default).}

\item{tree.imp}{logical;
use a prediction of missing data from single tree in the forest
when training (\code{T}) rather than the bagged predicted value
(default).}

\item{boot.train}{logical;
train each forest on a bootstrap sample of the observed data
when \code{T}, rather than the observed data (default).}

\item{obs.only}{logical;
train on only observed outcomes (default) or use all data
including predicted/sampled values of missing outcomes (\code{T}).}

\item{X.init.fn}{function;
creates a completed data set to be used as the initial state of
the missForest procedure given two arguments; \itemize{
\item a data.frame
\item a list with and item indicating the missing (\code{T})
or not-missing (\code{F}) status of each column of the
first argument,
} the default \code{\link{no_information_impute}} serves as an
example.}

\item{stop.measure}{function;
evaluates the difference or relationship between the two most
recently completed data sets during iteration, must accept the
following arguments;
\describe{
\item{\code{X}}{named list with imputed values (in order of
appearance by row) for each column in the data set;}
\item{\code{Y}}{named list with imputed values (in order of
appearance by row) for each column in the data set;}
\item{\code{X_init}}{the original (missed-type) data set with
missing values replaced as at the starting point of
missForest;}
\item{\code{indicator}}{a list with the missing (\code{=T})
or not missing (\code{=F}) status of the original data
set;}
} and should return a numeric (vector), the default
\code{\link{measure_correlation}} serves as an example, or see the
original measure proposed by Stekhoven and Buehlmann (2012) in
\code{\link{measure_stekhoven_2012}}.}

\item{loop.limit}{numeric;
maximum number of iterations within missForest procedure.}

\item{clean.step}{named list;
each item is a function to clean or post-process the named imputed
data immediately after it is imputed, taking two arguments;
\itemize{
\item the subset of the data used in the current training step
which had missing values of the named data,
\item the most recently imputed values of the named data,
} and should return (post-processed) data of the same length and
type as the second argument.}
}
\value{
list;
containing the following items; \describe{
\item{call}{the call used to create the multiply imputed
data sets;}
\item{results}{list where each item (numbered) is itself a
named list of the output for an imputed data set;
\describe{
\item{\code{converged}}{boolean convergence status;}
\item{\code{imputed}}{list of imputed data by
iteration and variable;}
\item{\code{iterations}}{numeric count of iterations
before stopping criteria met;}
\item{\code{oob_error}}{list of oob error by
iteration and variable;}
\item{\code{stop_measures}}{output of the call to
\code{stop.measure} at each iteration;}
}
}
\item{which_imputed}{named list of which rows the imputed
named data belong to.}
}
}
\description{
For a full description of the procedure for a single chain of 'mice' using
random forests see Doove et al (2014) and Bartlett (2014). In brief, the
procedure is:
}
\details{
\itemize{
\item For each variable (in a pre-specified order):
\itemize{
\item Train a random forest using the observed values.
\item For each missing case, traverse one or more trees and identify a pool
of candidate values from the leaves (for Doove et al, take the
observed from each leaf, for Bartlett et al, take bootstrap observed
values from one random leaf).
\item Draw once from the candidate values and update the variable.
}
}

Here, \code{data} must be complete; i.e. any missing value has already been
filled in some naive way using functions like \code{\link[=impute_naive_by_sample]{impute_naive_by_sample()}}
(the usual for 'mice') or \code{\link[=impute_naive_by_aggregate]{impute_naive_by_aggregate()}} (the original
'missForest' approach). \code{indicator} will dictate which values are considered
missing cases and will be updated by the loop.

\code{model} identifies which variables are included in each random forest. This
is either a numeric (with zero and one values) or logical matrix. See
\code{\link[=smirf]{smirf()}} for further details.

Training is performed by a call to \code{\link[literanger:train]{literanger::train()}} given by
\code{call_lr_train}, which may contain arguments applied to all random
forests. Per-variable arguments can be supplied using \code{overrides} which will
replace the 'global' arguments in \code{call_lr_train}. By default, classification
trees are used for factors, while regression trees are used for continuous
data.

\code{sampler} identifies whether or not a Gibbs-like sampler is employed
(\verb{='gibbs'}) over the variables or a 'missForest'-like sampler
(\verb{='missforest'}). The Gibbs-like sampler trains on the latest imputed
values, whereas the 'missForest'-like trains on the imputed values from the
previous iteration's complete data set. \code{prediction_type} set to \code{'inbag'}
uses Bartlett's prediction (2014), while \code{'doove'} uses the approach from
'mice' by Doove et al (2014). \code{'bagged'} can be used if estimating missing
values ala 'missForest'.

\code{stop_measure} can be set to one of \code{\link[=measure_correlation]{measure_correlation()}},
\code{\link[=measure_stekhoven_2012]{measure_stekhoven_2012()}}, or \code{\link[=measure_degenerate]{measure_degenerate()}}. The latter is
applicable to 'mice'-like algorithms and stops the loop when it reaches the
pre-determined \code{loop_limit}, while the former two are variations on stopping
criteria that can be used in 'missForest'-like estimation.

\code{clean_step} is a per-variable post-processing function called on each
variable after it has been imputed. It can be used to ensure that constraints
are applied to variables. Each item in the list is a function that accepts
\code{data} and \code{imputed}; where the former is the data-set prior to imputed
values being drawn (or the preceding complete data set if
\code{sampler='missforest'}) restricted to the missing cases (in order). The
function should return a vector the same length and type as \code{imputed} with
the clean values.

All the imputed values (over all iterations) are returned, along with; a
convergence indicator (if applicable); the number of iterations performed;
the out-of-bag error on a per-iteration and per-variable basis, and; the
recorded values of the stopping condition measures for each iteration.
Single or multiple imputation of missing data using random forests

Missing data (multiple) imputation using the missForest algorithm by
Stekhoven and Buehlmann (2012) (default) or, alternatively, the MICE with
random forest procedure of Doove et al (2014). The ranger (Wright and
Ziegler, 2017) fast implementation of random forest (training) algorithm is
used.

For a full description of the missForest algorithm, see Stekhoven and
Buehlmann (2012). In brief, at each iteration missing values are imputed for
each variable by the predictions of a random forest trained on the observed
cases of that variable using the values of predictors from the completed data
set from the previous iteration. This is repeated until some measure of the
relationship between iterations indicates convergence - usually by decreasing
from the measure at the previous iteration.

By default the columns are imputed in the order of least missing to most
missing. This can be over-ridden by the \code{model} argument. Columns that
are entirely missing are excluded. Non-integer numeric data is treated as
continuous and predicted by regression forests while all other data,
including integer and logical data, are predicted via classification forests.
No special treatment is given to ordered categorical data.

The call to \code{\link[ranger]{ranger}} may be modified by the \code{...}
arguments, and any variable-specific argument to pass may be specified in the
\code{overrides} argument.

The key modifications to the missForest procedure governed by the arguments:
\describe{
\item{\code{gibbs}}{use the most recent predictions for each variable
in training and prediction as they become available, like a Gibbs
sampler by setting this to \code{T} (default is \code{F});}
\item{\code{tree.imp}}{predict using a randomly selected tree for each
missing value rather than use the whole-of-forest aggregated
prediction by setting this to \code{T} (default is \code{F});}
\item{\code{boot.train}}{train on a boot-strapped resample of the data,
(default is \code{F}), and;}
\item{\code{obs.only}}{train on all rows in the data set by setting to
\code{F} or train on observed data only (default is \code{T}).}
}

Switching the first two to \code{T} invokes a similar procedure to Multiple
Imputation via Chained Equations of Doove et al (2014). The third option
can be used to improve CI coverage (Bartlett 2014). The final option (along
with changes to the first two) will mimic van Buuren and
Groothuis-Oudshoorn (2012), except for the process for drawing values from
leaf nodes.

The convergence criterion can be modified by the \code{stop.measure}
argument. The default is to measure the mean rank correlation between
iterations of the ordered data and the stationary rate of the categorical
data (see \code{\link{measure_correlation}}. The procedure halts when both of
these values are less than or equal to the previous values (see
\code{\link{stop_condition}}). The original Stekhoven and Buehlmann (2012)
measure is provided by the \code{\link{measure_stekhoven_2012}} function.
}
\references{
\itemize{
\item Bartlett, J. (2014). Methodology for multiple imputation for missing data
in electronic health record data, presented to \emph{27th International
Biometric Conference}, Florence, July 6-11.
\item Doove, L.L., Van Buuren, S., & Dusseldorp, E. (2014). Recursive
partitioning for missing data imputation in the presence of interaction
effects. \emph{Computational Statistics & Data Analysis}, 72, 92-104.
\doi{10.1016/j.csda.2013.10.025}.
\item Shah, A. D., Bartlett, J. W., Carpenter, J., Nicholas, O., & Hemingway,
H. (2014). Comparison of random forest and parametric imputation models
for imputing missing data using MICE: a CALIBER study. \emph{American journal
of epidemiology}, 179(6), 764-774. \doi{10.1093/aje/kwt312}.
\item Stekhoven, D. J. & Buehlmann, P. (2012). MissForest--non-parametric
missing value imputation for mixed-type data. \emph{Bioinformatics}, 28(1),
112-118. \doi{10.1093/bioinformatics/btr597}.
\item Wright, M. N., & Ziegler, A. (2017). ranger: A fast implementation of
random forests for high dimensional data in C++ and R. \emph{Journal of
Statistical Software}, 77(i01), 1-17. \doi{10.18637/jss.v077.i01}
}
}
\seealso{
\code{\link{measure_correlation}}
\code{\link{measure_stekhoven_2012}}
\code{\link{stop_condition}}
\code{\link{no_information_impute}} \code{\link{sample_impute}}
\code{\link[missForest]{missForest}}
\code{\link[ranger]{ranger}}

TODO: check CALIBERrfimpute - some settings differ!
also get up to speed with: https://academic.oup.com/aje/article/179/6/764/107562
}
